[
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#install-launch-r-packages-load-data-and-build-data-table",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#install-launch-r-packages-load-data-and-build-data-table",
    "title": "Take-home Exercise 1",
    "section": "4. Install, launch R packages, load data and build data table",
    "text": "4. Install, launch R packages, load data and build data table\n\n4.1 Reading the participants csv and build interactive paticipants table\n\n\nShow the code\npacman::p_load(tidyverse,dplyr,DT,plotly, ggdist,gganimate,ggstatsplot,ggiraph,ggridges, ggthemes)\n\n\n\n\nShow the code\np_data <- read_csv(\"data/Participants.csv\") %>% \n  mutate_if(is.character, as.factor) %>% \n  distinct()\nDT::datatable(p_data, class= \"compact\")\n\n\n\n\n\n\n\n\n\n \n\n\n4.2 Modify the time style of the financial data, pivot the catogory and regroup, calculate the Savings, and build interactive FinancialJournal table\n\n\nShow the code\nfin_data <- read_csv(\"data/FinancialJournal.csv\") %>% \n  mutate_if(is.character, as.factor) %>% distinct() \nfin_data$timestamp <- format(as.Date(fin_data$timestamp), \"%Y-%m\")\nf_data <- fin_data %>% group_by(participantId,timestamp,category) %>% \n  summarise(total_amount=sum(amount),\n            .groups = 'drop') %>%\n  as.data.frame()\nf_data <- f_data %>%\n  pivot_wider(.,names_from = \"category\", values_from = \"total_amount\") \nf_data <- replace(f_data, is.na(f_data), 0) %>%\n    mutate(Saving = select(., Education:RentAdjustment) %>% rowSums(na.rm = TRUE))\nDT::datatable(f_data, class= \"compact\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#analysis-and-visualization",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#analysis-and-visualization",
    "title": "Take-home Exercise 1",
    "section": "5. Analysis and visualization",
    "text": "5. Analysis and visualization\n\n5.1 Figure out the relationship of educationLevel and Joviality\n\nOverall, we see from below that with the increasing of education level, people tend to have higher level of joviality as the lower bound and mean value of joviality has a trend to increase. Although we can also observe that for low level of education, the range is bigger that all other level, this might indicate longer social experience can still affect the upper bound of joviality, but enhancing education is still a more direct way to increase joviality level.\n\nThe plotThe codeThe statistics table\n\n\n\n\n\n\n\n\n\n\ndf <- f_data %>% group_by(participantId) %>% \n  summarise(total_edu=sum(Education),\n            total_food=sum(Food),\n            total_rec=sum(Recreation),\n            total_shl=sum(Shelter),\n            total_w=sum(Wage),\n            total_rent=sum(RentAdjustment),\n            total_saving=sum(Saving),\n            .groups = 'drop') %>%\n  as.data.frame()\ndf2 <- merge(x= df,y= p_data, by = \"participantId\")\n\ndf3 <- fin_data %>%                              # Applying group_by & summarise\n  group_by(participantId) %>%\n  summarise(count_month = n_distinct(timestamp)) %>%\n  filter(count_month>1)\ndf4 <-merge(x=df3,y=df2, \n             by=\"participantId\", all.x=TRUE)\n\nmy_sum <- df4 %>%\n  group_by(educationLevel) %>%\n  summarise(\n    n=n(),\n    mean=mean(joviality),\n    sd=sd(joviality)\n    ) %>%\n  mutate(se=sd/sqrt(n-1))\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=educationLevel, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=educationLevel, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  scale_x_discrete(limits = c(\"Low\", \"HighSchoolOrCollege\", \"Bachelors\", \"Graduate\")) +\n  ggtitle(\"Standard error of mean \n          Joviality score by educationLevel\")\n\n\n\n\nsumm <- my_sum[order(factor(my_sum$educationLevel, levels = c(\"Low\", \"HighSchoolOrCollege\", \"Bachelors\", \"Graduate\"))),c(\"educationLevel\",\"n\",\"mean\",\"sd\")]\nknitr::kable(head(summ), format = 'html')\n\n\n\n \n  \n    educationLevel \n    n \n    mean \n    sd \n  \n \n\n  \n    Low \n    56 \n    0.4187639 \n    0.2937796 \n  \n  \n    HighSchoolOrCollege \n    422 \n    0.4346854 \n    0.2724155 \n  \n  \n    Bachelors \n    232 \n    0.4975164 \n    0.2976517 \n  \n  \n    Graduate \n    170 \n    0.5162219 \n    0.2969368 \n  \n\n\n\n\n\n\n\n\n\n\n\n5.2 Figure out the relationship of householdSize and Joviality\n\nFrom the plotting below, we see that number of householdSize does not have huge effect on the median of Joviality but there is still some slight impact that household with 2 or more people tend to have a middle or low level of joviality while one person are more intend to be either with high or low joviality.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = df4,\n  x = householdSize, \n  y = joviality,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n5.3 Savings and Joviality\n\nfrom the graph below, we can find that the saving of most people are below 50000 within the study period.\n\n\nShow the code\ngghistostats(\n  data = df4,\n  x = total_saving,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"total_saving\"\n)\n\n\n\n\n\n\n\nConsider the Engel’s coefficient, which is simply a food budget share, it is used for this purpose by The United Nations (UN), where a coefficient above 59% represents poverty, 50-59% represents a state where daily needs are barely met, 40-50% a moderately well-off standard of living, 30-40% a good standard of living and below 30% a wealthy life. It can be seem here overall the Engel’s coefficients of the households do not have significant change, but there are still some household above 60%.\n\n\nShow the code\ndf5 <- data.frame(f_data)\ndf5$RentAdjustment[df5$RentAdjustment>0] <- 0\ndf5 <- df5[df5$timestamp!='2022-03',]\ndf5 <- df5[df5$timestamp!='2023-02',]\ndf5$cost <- df5$Education+df5$Food+df5$Recreation+df5$Shelter+df5$RentAdjustment\n\ndf6 <- df5 %>% group_by(participantId, timestamp) %>% \n  summarise(mon_saving=sum(Saving),\n            mon_cost=sum(cost),\n            mon_food=sum(Food),\n            .groups = 'drop') %>%\n  as.data.frame()\n\ngg <- ggplot(df6, \n       aes(x = -mon_food, \n           y = mon_food/mon_cost, \n           size = -mon_cost, \n           colour = participantId)) +\n  geom_point(aes(size = -mon_cost,\n                 frame = timestamp),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_size(range = c(1, 5)) +\n  labs(x = 'mon_food', \n       y = 'Engel\\'s Coefficient')\n\nggplotly(gg)\n\n\n\n\n\n\n\n\n\n5.4 InterestGroup and Joviality\n\nIt seems that most interest groups has a mean joviality value around 0.4 and has similar joviality distribution, but groups like D or H should be paid more attention to. But overall, interest groups promote freedom of expression, explore new perspectives, and allow people of all level of joviality to gather, to share and to express.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\nggplot(df4, \n       aes(x = interestGroup, \n           y = joviality)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA,\n               slab_fill = \"light blue\") +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#figure-out-the-relationship-of-householdsize-and-joviality",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#figure-out-the-relationship-of-householdsize-and-joviality",
    "title": "Take-home Exercise 1",
    "section": "5.2 Figure out the relationship of householdSize and Joviality",
    "text": "5.2 Figure out the relationship of householdSize and Joviality\n\nFrom the plotting below, we see that number of householdSize does not have huge effect on Joviality but there is still some slight impact that household with 2 or more people tend to have a bit higher mean joviality than 1 person and 2 people have the hight mean.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = df2,\n  x = householdSize, \n  y = joviality,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#next",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#next",
    "title": "Take-home Exercise 1",
    "section": "Next",
    "text": "Next\n\ngghistostats(\n  data = df2,\n  x = total_saving,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"total_saving\"\n)\n\n\n\n\n\nplot_ly(data = df2, \n        x = ~total_saving, \n        y = ~joviality, \n        color = ~interestGroup)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#distribution-of-savings",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#distribution-of-savings",
    "title": "Take-home Exercise 1",
    "section": "Distribution of savings",
    "text": "Distribution of savings\n\nfrom the graph below, we can find that the saving of most people are below 50000 within the study period.\n\ngghistostats(\n  data = df2,\n  x = total_saving,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"total_saving\"\n)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#and-by-plotting-the-ralationship-of-savings-interestgroups-and-joviality-we-can-see-that-no-matter-how-much-people-save-they-are-still-willing-to-join-any-kinds-of-interest-groups.",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#and-by-plotting-the-ralationship-of-savings-interestgroups-and-joviality-we-can-see-that-no-matter-how-much-people-save-they-are-still-willing-to-join-any-kinds-of-interest-groups.",
    "title": "Take-home Exercise 1",
    "section": "And by plotting the ralationship of savings, interestGroups and joviality, we can see that no matter how much people save, they are still willing to join any kinds of interest groups.",
    "text": "And by plotting the ralationship of savings, interestGroups and joviality, we can see that no matter how much people save, they are still willing to join any kinds of interest groups.\n\nplot_ly(data = df2, \n        x = ~total_saving, \n        y = ~joviality, \n        color = ~interestGroup)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#conclusion",
    "title": "Take-home Exercise 1",
    "section": "Conclusion",
    "text": "Conclusion\n\nTherefore, in order to assist with the major community revitalization efforts, the very large city renewal grant they have recently received should be allocated as below:\n\n 1.Invest in high-quality teachers, provide resources and support for schools and universities and utilize technology like online learning platforms and educational software to encourage people to gain knowledge and improve education qualification\n\n\n 2.Help poor people to gain more skills and develop working opportunities for them to increase their salary. Increase the overall Engel’s Coefficient value to reduce proverty and improve the living condition of households.\n\n\n 3.Make dating apps, matchmaking services, holding networking events or other method to help single people conveniently meet and connect with like-minded people and find a partner\n\n\n 4.Setup community service, hold activities and interest groups to provide people with opportunities to meet people of same interests."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands-on Exercise 5",
    "section": "",
    "text": "pacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\n\n\n\nGAStech_nodes <- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges <- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\n\n\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\nGAStech_edges <- GAStech_edges %>%\n  mutate(SendDate = dmy(SentDate)) %>%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\nGAStech_edges_aggregated <- GAStech_edges %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(source, target, Weekday) %>%\n    summarise(Weight = n()) %>%\n  filter(source!=target) %>%\n  filter(Weight > 1) %>%\n  ungroup()\n\n\n\n\n\n\n\n\nGAStech_graph <- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 54 × 4\n     id label               Department     Title                                \n  <dbl> <chr>               <chr>          <chr>                                \n1     1 Mat.Bramar          Administration Assistant to CEO                     \n2     2 Anda.Ribera         Administration Assistant to CFO                     \n3     3 Rachel.Pantanal     Administration Assistant to CIO                     \n4     4 Linda.Lagos         Administration Assistant to COO                     \n5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Manag…\n6     6 Carla.Forluniau     Administration Assistant to IT Group Manager        \n# ℹ 48 more rows\n#\n# A tibble: 1,372 × 4\n   from    to Weekday Weight\n  <int> <int> <ord>    <int>\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\n\n\n\n\nGAStech_graph %>%\n  activate(edges) %>%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 1,372 × 4\n   from    to Weekday  Weight\n  <int> <int> <ord>     <int>\n1    40    41 Saturday     13\n2    41    43 Monday       11\n3    35    31 Tuesday      10\n4    40    41 Monday       10\n5    40    43 Monday       10\n6    36    32 Sunday        9\n# ℹ 1,366 more rows\n#\n# A tibble: 54 × 4\n     id label           Department     Title           \n  <dbl> <chr>           <chr>          <chr>           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\n\n\n\n\n\n\n\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\ng <- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\ng <- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n\ng <- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\nset_graph_style() \n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\n\ng <- GAStech_graph %>%\n  mutate(betweenness_centrality = centrality_betweenness()) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\ng <- GAStech_graph %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\n\ng <- GAStech_graph %>%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nGAStech_edges_aggregated <- GAStech_edges %>%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %>%\n  rename(from = id) %>%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %>%\n  rename(to = id) %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(from, to) %>%\n    summarise(weight = n()) %>%\n  filter(from!=to) %>%\n  filter(weight > 1) %>%\n  ungroup()\n\n\n\n\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\n\n\n\n\nGAStech_nodes <- GAStech_nodes %>%\n  rename(group = Department) \n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "Welcome to ISSS608 Visual Analytics and Application. In this website, you will find my coursework prepared for this course."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01 - v2.html#install-launch-r-packages-load-data-and-build-data-table",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01 - v2.html#install-launch-r-packages-load-data-and-build-data-table",
    "title": "Take-home Exercise 1",
    "section": "4. Install, launch R packages, load data and build data table",
    "text": "4. Install, launch R packages, load data and build data table\n\nReading the paticipants csv and build interactive paticipants table\n\n\nShow the code\npacman::p_load(tidyverse,dplyr,DT,plotly, ggdist,gganimate,ggstatsplot,ggiraph)\n\n\n\n\nShow the code\np_data <- read_csv(\"data/Participants.csv\") %>% \n  mutate_if(is.character, as.factor) %>% \n  distinct()\nDT::datatable(p_data, class= \"compact\")\n\n\n\n\n\n\n\n\n\nModify the time style of the financial data, pivot the catogory and regroup, calculate the Savings, and build interactive FinancialJournal table\n\n\nShow the code\nfin_data <- read_csv(\"data/FinancialJournal.csv\") %>% \n  mutate_if(is.character, as.factor) %>% distinct() \nfin_data$timestamp <- format(as.Date(fin_data$timestamp), \"%Y-%m\")\nf_data <- fin_data %>% group_by(participantId,timestamp,category) %>% \n  summarise(total_amount=sum(amount),\n            .groups = 'drop') %>%\n  as.data.frame()\nf_data <- f_data %>%\n  pivot_wider(.,names_from = \"category\", values_from = \"total_amount\") \nf_data <- replace(f_data, is.na(f_data), 0) %>%\n    mutate(Saving = select(., Education:RentAdjustment) %>% rowSums(na.rm = TRUE))\nDT::datatable(f_data, class= \"compact\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01 - v2.html#analysis-and-visualization",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01 - v2.html#analysis-and-visualization",
    "title": "Take-home Exercise 1",
    "section": "5. Analysis and visualization",
    "text": "5. Analysis and visualization\n\n5.1 Figure out the relationship of educationLevel and Joviality\n\nOverall, we see from below that with the increasing of education level, people tend to have higher level of joviality as the lower bound and mean value of joviality has a trend to increase. Although we can also observe that for low level of education, the range is bigger that all other level, this might indicate longer social experience can still affect the upper bound of joviality, but enhancing education is still a more direct way to increase joviality level.\n\nThe plotThe codeThe statistics table\n\n\n\n\n\n\n\n\n\n\ndf <- f_data %>% group_by(participantId) %>% \n  summarise(total_edu=sum(Education),\n            total_food=sum(Food),\n            total_rec=sum(Recreation),\n            total_shl=sum(Shelter),\n            total_w=sum(Wage),\n            total_rent=sum(RentAdjustment),\n            total_saving=sum(Saving),\n            .groups = 'drop') %>%\n  as.data.frame()\ndf2 <- merge(x= df,y= p_data, by = \"participantId\")\n\ndf3 <- fin_data %>%                              # Applying group_by & summarise\n  group_by(participantId) %>%\n  summarise(count_month = n_distinct(timestamp)) %>%\n  filter(count_month>1)\ndf4 <-merge(x=df3,y=df2, \n             by=\"participantId\", all.x=TRUE)\n\nmy_sum <- df4 %>%\n  group_by(educationLevel) %>%\n  summarise(\n    n=n(),\n    mean=mean(joviality),\n    sd=sd(joviality)\n    ) %>%\n  mutate(se=sd/sqrt(n-1))\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=educationLevel, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=educationLevel, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  scale_x_discrete(limits = c(\"Low\", \"HighSchoolOrCollege\", \"Bachelors\", \"Graduate\")) +\n  ggtitle(\"Standard error of mean \n          Joviality score by educationLevel\")\n\n\n\n\nsumm <- my_sum[order(factor(my_sum$educationLevel, levels = c(\"Low\", \"HighSchoolOrCollege\", \"Bachelors\", \"Graduate\"))),c(\"educationLevel\",\"n\",\"mean\",\"sd\")]\nknitr::kable(head(summ), format = 'html')\n\n\n\n \n  \n    educationLevel \n    n \n    mean \n    sd \n  \n \n\n  \n    Low \n    56 \n    0.4187639 \n    0.2937796 \n  \n  \n    HighSchoolOrCollege \n    422 \n    0.4346854 \n    0.2724155 \n  \n  \n    Bachelors \n    232 \n    0.4975164 \n    0.2976517 \n  \n  \n    Graduate \n    170 \n    0.5162219 \n    0.2969368 \n  \n\n\n\n\n\n\n\n\n\n\n\n5.2 Figure out the relationship of householdSize and Joviality\n\nFrom the plotting below, we see that number of householdSize does not have huge effect on the median of Joviality but there is still some slight impact that household with 2 or more people tend to have a middle or low level of joviality while one person are more intend to be either with high or low joviality\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = df4,\n  x = householdSize, \n  y = joviality,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n5.3 Money and Joviality\n\nfrom the graph below, we can find that the saving of most people are below 50000 within the study period.\n\n\nShow the code\ngghistostats(\n  data = df4,\n  x = total_saving,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"total_saving\"\n)\n\n\n\n\n\n\n\n\nAnd by plotting the ralationship of wages and joviality, we can high wages does not really mean for high joviality.\n\n\nShow the code\nggscatterstats(\n  data = df4,\n  x = total_w,\n  y = joviality,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n5.4 InterestGroup and Joviality\n\nIt seems that most interest groups has a mean joviality value around 0.5 and has similar joviality distribution, but groups like D or H should be paid more attention to.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\ntooltip <- function(y, ymax, accuracy = .01) {\n  mean <- scales::number(y, accuracy = accuracy)\n  sem <- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean Joviality scores:\", mean, \"+/-\", sem)\n}\n\ngg_point <- ggplot(data=df4, \n                   aes(x = interestGroup),\n) +\n  stat_summary(aes(y = joviality, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = joviality),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )+\n  coord_cartesian(ylim=c(0,0.8))\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01 - v2.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01 - v2.html#conclusion",
    "title": "Take-home Exercise 1",
    "section": "Conclusion",
    "text": "Conclusion\n\nTherefore, in order to assist with the major community revitalization efforts, the very large city renewal grant they have recently received should be allocated as below:\n\n Invest in high-quality teachers, provide resources and support for schools and universities and utilize technology like online learning platforms and educational software to encourage people to gain knowledge and improve education qualification\n\n\n Make dating apps, matchmaking services, holding networking events or other method to help single people conveniently meet and connect with like-minded people and find a partner\n\n\n Setup community service, hold activities and interest groups to provide people with opportunities to meet people of same interests."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "In-class Exercise 5",
    "section": "",
    "text": "pacman::p_load(jsonlite,tidygraph,ggraph, visNetwork,tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#load-data",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#load-data",
    "title": "In-class Exercise 5",
    "section": "Load data",
    "text": "Load data\n\nMC1 <- jsonlite::fromJSON(\"data/MC1.json\")\n\n\nExpand MC1_nodes\n\nMC1_nodes <- as_tibble(MC1$nodes) %>%\n  select(id, type, country)\n\n\n\nExpand MC1_edges\n\nMC1_edges <- as_tibble(MC1$links) %>%\n  select(source,target,type,weight,key)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#the-task",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#the-task",
    "title": "Take-home Exercise 1",
    "section": "2. The Task",
    "text": "2. The Task\n\nIn this take-home exercise, you are required to apply the concepts and methods you had learned in Lesson 1-4 to reveal the demographic and financial characteristics of the city of Engagement, using appropriate static and interactive statistical graphics methods. This exercise requires a user-friendly and interactive solution that helps city managers and planners to explore the complex data in an engaging way and reveal hidden patterns. The data should be processed by using appropriate tidyverse family of packages and the statistical graphics must be prepared using ggplot2 and its extensions."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-description",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-description",
    "title": "Take-home Exercise 1",
    "section": "3. Data Description",
    "text": "3. Data Description\n\nFor the purpose of this study, two data sets are provided. They are:\n\n\nParticipants.csv: Contains information about the residents of City of Engagement that have agreed to participate in this study.\n\n\n\n\n\n\nDetails of the csv\n\n\n\n\n\n  participantId (integer): unique ID assigned to each participant.\n  householdSize (integer): the number of people in the participant’s household\n  haveKids (boolean): whether there are children living in the participant’s household.\n  age (integer): participant’s age in years at the start of the study.\n  educationLevel (string factor): the participant’s education level, one of: {“Low”, “HighSchoolOrCollege”, “Bachelors”, “Graduate”}\n  interestGroup (char): a char representing the participant’s stated primary interest group, one of {“A”, “B”, “C”, “D”, “E”, “F”, “G”, “H”, “I”, “J”}. Note: specific topics of interest have been redacted to avoid bias.\n  joviality (float): a value ranging from [0,1] indicating the participant’s overall happiness level at the start of the study.\n\n\n\n\n\nFinancialJournal.csv: Contains information about financial transactions.\n\n\n\n\n\n\nDetails of the csv\n\n\n\n\n\n  participantId (integer): unique ID corresponding to the participant affected\n  timestamp (datetime): the time when the check-in was logged\n  category (string factor): a string describing the expense category, one of {“Education”, “Food”, “Recreation”, “RentAdjustment”, “Shelter”, “Wage”}\n  amount (double): the amount of the transaction"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "title": "In-class Exercise 4",
    "section": "",
    "text": "pacman::p_load(rstatix, gt, patchwork, tidyverse)\nexam_data <- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(sample=ENGLISH))+\n  stat_qq() + \n  stat_qq_line()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can see that the points deviate significantly from the straight diagonal line. This is a clear indication that the set of data is not normally distributed.\n\n\n\n\n\n\nThe plotThe code trunk\n\n\n\n\n\n\n\n\n\n\nqq <- ggplot(exam_data, aes(sample=ENGLISH))+\n  stat_qq() + \n  stat_qq_line()\n\nsw_t <- exam_data %>%\n  shapiro_test(ENGLISH) %>%\n  gt()\n\ntmp <- tempfile(fileext = '.png')\ngtsave(sw_t, tmp)\ntable_png <- png::readPNG(tmp,native=TRUE)\n\nqq + table_png"
  },
  {
    "objectID": "Take-home_Ex/Take_home_Ex02/Take_home_Ex02.html#install-packages-load-data-and-data-wrangling",
    "href": "Take-home_Ex/Take_home_Ex02/Take_home_Ex02.html#install-packages-load-data-and-data-wrangling",
    "title": "Take-home Exercise 2",
    "section": "2.Install packages, load data and data wrangling",
    "text": "2.Install packages, load data and data wrangling\n\n2.1 Load packages and data\n\npacman::p_load(jsonlite,tidygraph, visNetwork,\n               ggraph, tidyr, ggstatsplot,\n               lubridate, plotly, gganimate, plotly,igraph,\n               DT, tidyverse)\n\n\nMC2 <- jsonlite::fromJSON(\"data/mc2_challenge_graph.json\")\n\n\n\n2.2 Expand MC2 nodes and links\n\nMC2_nodes <- as_tibble(MC2$nodes) %>%\n  distinct() %>%\n  select(id, shpcountry, rcvcountry)\n\nMC2_links <- as_tibble(MC2$links) %>%\n  mutate(ArrivalDate = ymd(arrivaldate)) %>%\n  mutate(Year = year(ArrivalDate)) %>%\n  select(source, target, arrivaldate, Year, hscode, valueofgoods_omu, volumeteu, weightkg, valueofgoodsusd) %>%\n  distinct()\n\n\n\n2.3 Reviewing the imported data\n\nNodes\n\n\nShow the code\nglimpse(MC2_nodes)\n\n\nRows: 34,576\nColumns: 3\n$ id         <chr> \"AquaDelight Inc and Son's\", \"BaringoAmerica Marine Ges.m.b…\n$ shpcountry <chr> \"Polarinda\", NA, \"Oceanus\", NA, \"Oceanus\", \"Kondanovia\", NA…\n$ rcvcountry <chr> \"Oceanus\", NA, \"Oceanus\", NA, \"Oceanus\", \"Utoporiana\", NA, …\n\n\n\n\nEdges\n\n\nShow the code\nglimpse(MC2_links)\n\n\nRows: 5,309,087\nColumns: 9\n$ source           <chr> \"AquaDelight Inc and Son's\", \"AquaDelight Inc and Son…\n$ target           <chr> \"BaringoAmerica Marine Ges.m.b.H.\", \"BaringoAmerica M…\n$ arrivaldate      <chr> \"2034-02-12\", \"2034-03-13\", \"2028-02-07\", \"2028-02-23…\n$ Year             <dbl> 2034, 2034, 2028, 2028, 2028, 2028, 2028, 2028, 2028,…\n$ hscode           <chr> \"630630\", \"630630\", \"470710\", \"470710\", \"470710\", \"47…\n$ valueofgoods_omu <dbl> 141015, 141015, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ volumeteu        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ weightkg         <int> 4780, 6125, 10855, 11250, 11165, 11290, 9000, 19490, …\n$ valueofgoodsusd  <dbl> NA, NA, NA, NA, NA, NA, 87110, 188140, NA, 221110, 58…\n\n\n\n\n\n2.4 Data preview\n\n2.4.1 Preparation for aggregated nodes and edges and select an example hscode and year to create a graph\n\n\nShow the code\nMC2_edges_aggregated <- MC2_links %>%\n  left_join(MC2_nodes, by = c(\"source\" = \"id\")) %>%\n  rename(from = source) %>%\n  left_join(MC2_nodes, by = c(\"target\" = \"id\")) %>%\n  rename(to = target) %>%\n  filter(hscode==\"306170\" & Year==\"2028\")%>%\n  group_by(from, to) %>%\n    summarise(weight = n()) %>%\n  filter(from!=to) %>%\n  filter(weight > 20) %>%\n  ungroup()\nMC2_nodes_aggregated <- MC2_nodes %>%\n  filter(id %in% c(MC2_edges_aggregated$from, MC2_edges_aggregated$to))\n\n\n\nDT::datatable(MC2_nodes_aggregated, class= \"compact\")\n\n\n\n\n\n\n\nDT::datatable(MC2_edges_aggregated, class= \"compact\")\n\n\n\n\n\n\n\n\n2.4.2 Plot the graph\n\n\nShow the code\n#set.seed(123)\n#edges <- MC2_edges_aggregated #%>% sample_frac(0.005)\n#nodes <- MC2_nodes %>%\n  #filter(id %in% c(edges$from, edges$to))\nvisNetwork(MC2_nodes_aggregated,\n           MC2_edges_aggregated ) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\"))"
  },
  {
    "objectID": "Take-home_Ex/Take_home_Ex02/Take_home_Ex02.html#visualization-and-analysis",
    "href": "Take-home_Ex/Take_home_Ex02/Take_home_Ex02.html#visualization-and-analysis",
    "title": "Take-home Exercise 2",
    "section": "3. Visualization and Analysis",
    "text": "3. Visualization and Analysis\n\nAn overview of the temporal change of sampled edges within 2028 to 2034\n\nHere we extract the edge data and group by year-month, and we can see the changes of the sum of weight and value_usd on a monthly changed base.Mostly the sum of weight is below 100k and value_usd below 0.6M.\n\n\nShow the code\nMC2_links$arrivalym <-  format(as.Date(MC2_links$arrivaldate),\"%Y-%m\")\nMC2_edges_5 <- MC2_links %>% group_by(source,target,arrivalym) %>% \n  summarise(count_hscode=n_distinct(hscode),\n            sum_omu=sum(valueofgoods_omu),\n            sum_teu=sum(volumeteu),\n            sum_weightkg=sum(weightkg),\n            sum_usd=sum(valueofgoodsusd),.groups = 'drop') %>%\n  \n  as.data.frame()\nMC2_edges_5 <- MC2_edges_5[complete.cases(MC2_edges_5[ , c('sum_weightkg', 'sum_usd')]), ]\nset.seed(123)\nMC2_nodes_5 <- MC2_nodes %>%\n  filter(id %in% MC2_edges_5$source) %>% sample_frac(0.005)\nMC2_edges_6 <- MC2_edges_5 %>%\n  filter(MC2_edges_5$source %in% MC2_nodes_5$id) %>%\n  filter(sum_weightkg< 2000000) %>%\n  filter(sum_usd< 1000000)\n\n  \nbp <- MC2_edges_6 %>%\n  plot_ly(x = ~sum_weightkg, \n          y = ~sum_usd, \n          size = ~sum_weightkg, \n          color = ~source,\n          sizes = c(2, 100),\n          frame = ~arrivalym, \n          text = ~target, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %>%\n  layout(showlegend = FALSE)\nbp\n\n\n\n\n\n\n\n\n\nComparison of distribution of mean avg_price records that have 1-7 years of history.\n\nHere we pay attention to a yearly change in below and we found that companies that has only 1 year of record has the highest avg_price. Considering if a company is legally fishing, mostly they will form a long relationship with collaborators and the price is reasonable. But for companies that are caught illegal fishing or shut down, the price are more likely to be higher than the market and their record histories are also short.\n\n\nShow the code\nMC2_edges <- MC2_links[complete.cases(MC2_links[ , c('hscode','valueofgoodsusd','volumeteu')]), ] %>% \n  filter(hscode == \"306170\") %>%\n  filter(weightkg != 0 ) %>% \n  mutate(price = valueofgoodsusd/weightkg)\n\nMC2_edges_1 <- MC2_edges %>% \n  group_by(source,target,Year) %>% \n  summarise(No_of_time=n(),\n            sum_omu=sum(valueofgoods_omu),\n            sum_teu=sum(volumeteu),\n            sum_weightkg=sum(weightkg),\n            sum_usd=sum(valueofgoodsusd),\n            mean_price=mean(price),.groups = 'drop') %>%\n  as.data.frame()\n\nMC2_edges_2 <- MC2_edges_1 %>% group_by(source,target) %>% \n  summarise(No_of_year=n_distinct(Year),\n            avg_not=mean(No_of_time),\n            avg_price=mean(mean_price),\n            avg_weightkg=mean(sum_weightkg),.groups = 'drop') %>%\n  #inner_join(edges, by = c(\"source\"=\"from\",\"target\"=\"to\")) %>%\n  as.data.frame()\n\n\nmy_sum <- MC2_edges_2 %>%\n  group_by(No_of_year) %>%\n  summarise(\n    n=n(),\n    mean=mean(avg_price),\n    sd=sd(avg_price)\n    ) %>%\n  mutate(se=sd/sqrt(n-1))\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=No_of_year, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=No_of_year, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean avg_price score by No_of_year\")\n\n\n\n\n\n\n\nStats table\n\n\nShow the code\nknitr::kable(my_sum, format = 'html')\n\n\n\n\n \n  \n    No_of_year \n    n \n    mean \n    sd \n    se \n  \n \n\n  \n    1 \n    4387 \n    10.93051 \n    4.400719 \n    0.0664491 \n  \n  \n    2 \n    1410 \n    10.70853 \n    2.746642 \n    0.0731723 \n  \n  \n    3 \n    713 \n    10.82606 \n    2.299127 \n    0.0861634 \n  \n  \n    4 \n    358 \n    10.66151 \n    1.886805 \n    0.0998603 \n  \n  \n    5 \n    250 \n    10.60475 \n    2.499360 \n    0.1583905 \n  \n  \n    6 \n    225 \n    10.58984 \n    1.293865 \n    0.0864500 \n  \n  \n    7 \n    207 \n    10.60920 \n    1.082008 \n    0.0753871 \n  \n\n\n\n\n\n\n\n\nFurther analysis on short-history edges\n\nHere we filter with year=1 and plot out 10%\n\n\nShow the code\nMC2_edges_2_aggregated <- MC2_edges_2 %>%\n  left_join(MC2_nodes, by = c(\"source\" = \"id\")) %>%\n  rename(from = source) %>%\n  left_join(MC2_nodes, by = c(\"target\" = \"id\")) %>%\n  rename(to = target) %>%\n  filter(No_of_year==1)%>%\n  filter(from!=to) %>%\n  filter(avg_not>5)%>%\n  ungroup()\nMC2_nodes_2_aggregated <- MC2_nodes %>%\n  filter(id %in% c(MC2_edges_2_aggregated$from, MC2_edges_2_aggregated$to))\n\nvisNetwork(MC2_nodes_2_aggregated,\n           MC2_edges_2_aggregated ) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\"))\n\n\n\n\n\n\n\n\nthe top 10% vertices based on their eigenvector centrality score\n\n\nShow the code\ncgraph <- graph_from_data_frame (MC2_edges_2_aggregated,\n                           vertices = MC2_nodes_2_aggregated) %>%\n  as_tbl_graph()\nquantile_graph <- quantile(eigen_centrality(cgraph)$vector,\n         probs = seq(0, 1, 1/10)\n         )\nV(cgraph)$size = eigen_centrality(cgraph)$vector\n\ncgraph_aggregated <- delete_vertices(cgraph, V(cgraph)[size < quantile_graph[10]])\n\n\nset.seed (1234)\nlayout1 <- layout_with_fr(cgraph_aggregated)\n\nquantile_graph_aggregated <- quantile(V(cgraph_aggregated)$size, #identify top 10% of the new vertices\n         probs = seq(0, 1, 1/10)\n         )\n\n\nV(cgraph_aggregated)$color <- ifelse (V(cgraph_aggregated)$size > quantile_graph_aggregated[10], \"darkgoldenrod3\", \"azure3\") #color yellow if vertices is top 10%\nE(cgraph_aggregated)$color <- \"grey\"\nV(cgraph_aggregated)$size <- V(cgraph_aggregated)$size/0.065 \n#Increase the size of nodes based on their centrality score, only those with high score will be visible\n\nV(cgraph_aggregated)$label <- ifelse (V(cgraph_aggregated)$size*0.065 > quantile_graph_aggregated[10],V(cgraph_aggregated)$name,NA)\n#label the vertices if vertices belongs to the top 10%\n\nplot(cgraph_aggregated, edge.arrow.size=0.25,edge.arrow.mode = \"-\", vertex.label = V(cgraph_aggregated)$label, vertex.label.cex = 0.65, vertex.label.font = 1, main = \"Which IDs has the most shipments within 1 year?\" ) \n\n\n\n\n\n\n\nHere we have the attributes of the top 10% vertices based on the updated EigenVector Centrality.\n\n\nShow the code\nvertex_attr(cgraph_aggregated, index = V(cgraph_aggregated)$size*0.065 > quantile_graph_aggregated[10])\n\n\n$name\n[1] \"hǎi dǎn Corporation Wharf\" \"Mar del Este CJSC\"        \n[3] \"Caracola del Sol Services\" \"Coral del Sur Sagl Family\"\n\n$shpcountry\n[1] \"Marebak\"     \"Merigrad\"    \"Puerto Sol\"  \"Solovarossa\"\n\n$rcvcountry\n[1] \"Oceanus\" \"Oceanus\" \"Oceanus\" \"Oceanus\"\n\n$size\n[1]  7.920180 15.384615 12.956742  8.635514\n\n$color\n[1] \"darkgoldenrod3\" \"darkgoldenrod3\" \"darkgoldenrod3\" \"darkgoldenrod3\"\n\n$label\n[1] \"hǎi dǎn Corporation Wharf\" \"Mar del Este CJSC\"        \n[3] \"Caracola del Sol Services\" \"Coral del Sur Sagl Family\"\n\n\n\n\ndetect possible cluster based on the betweeness score of the vertices\n\n\nShow the code\nGNC <- cluster_edge_betweenness(cgraph_aggregated, weights = NULL)\nV(cgraph_aggregated)$color <-membership(GNC)              \ncgraph_aggregated$palette <- diverging_pal(length(GNC)) \nplot(cgraph_aggregated, edge.arrow.size=0.25,edge.arrow.mode = \"-\", vertex.label = NA, main = \"How many Social Groups within the category?\")\n\n\n\n\n\n\n\n\nHere according to the results above, we can make an hypothesis that since FishEye knows from past experience that companies caught fishing illegally will shut down but will then often start up again under a different name, and we find out that companies that have only 1 year of transactions history ofter have the highest avg_price, it is very likely that among these, the trade records may have problems. And we focus on top 10% as it is weird that these companies disappeared in the second year if they fish legally since the trade record is more frequent. These companies and records should be well-studied to further find out more IUU fishing behaviors."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#install-packages-load-data-and-data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#install-packages-load-data-and-data-wrangling",
    "title": "Take-home Exercise 2",
    "section": "2.Install packages, load data and data wrangling",
    "text": "2.Install packages, load data and data wrangling\n\n2.1 Load packages and data\n\npacman::p_load(jsonlite,tidygraph, visNetwork,\n               ggraph, tidyr, ggstatsplot,\n               lubridate, plotly, gganimate, plotly,igraph,\n               DT, tidyverse)\n\n\nMC2 <- jsonlite::fromJSON(\"data/mc2_challenge_graph.json\")\n\n\n\n2.2 Expand MC2 nodes and links\n\nMC2_nodes <- as_tibble(MC2$nodes) %>%\n  distinct() %>%\n  select(id, shpcountry, rcvcountry)\n\nMC2_links <- as_tibble(MC2$links) %>%\n  mutate(ArrivalDate = ymd(arrivaldate)) %>%\n  mutate(Year = year(ArrivalDate)) %>%\n  select(source, target, arrivaldate, Year, hscode, valueofgoods_omu, volumeteu, weightkg, valueofgoodsusd) %>%\n  distinct()\n\n\n\n2.3 Reviewing the imported data\n\nNodes\n\n\nShow the code\nglimpse(MC2_nodes)\n\n\nRows: 34,576\nColumns: 3\n$ id         <chr> \"AquaDelight Inc and Son's\", \"BaringoAmerica Marine Ges.m.b…\n$ shpcountry <chr> \"Polarinda\", NA, \"Oceanus\", NA, \"Oceanus\", \"Kondanovia\", NA…\n$ rcvcountry <chr> \"Oceanus\", NA, \"Oceanus\", NA, \"Oceanus\", \"Utoporiana\", NA, …\n\n\n\n\nEdges\n\n\nShow the code\nglimpse(MC2_links)\n\n\nRows: 5,309,087\nColumns: 9\n$ source           <chr> \"AquaDelight Inc and Son's\", \"AquaDelight Inc and Son…\n$ target           <chr> \"BaringoAmerica Marine Ges.m.b.H.\", \"BaringoAmerica M…\n$ arrivaldate      <chr> \"2034-02-12\", \"2034-03-13\", \"2028-02-07\", \"2028-02-23…\n$ Year             <dbl> 2034, 2034, 2028, 2028, 2028, 2028, 2028, 2028, 2028,…\n$ hscode           <chr> \"630630\", \"630630\", \"470710\", \"470710\", \"470710\", \"47…\n$ valueofgoods_omu <dbl> 141015, 141015, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ volumeteu        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ weightkg         <int> 4780, 6125, 10855, 11250, 11165, 11290, 9000, 19490, …\n$ valueofgoodsusd  <dbl> NA, NA, NA, NA, NA, NA, 87110, 188140, NA, 221110, 58…\n\n\n\n\n\n2.4 Data preview\n\n2.4.1 Preparation for aggregated nodes and edges and select an example hscode and year to create a graph\n\n\nShow the code\nMC2_edges_aggregated <- MC2_links %>%\n  left_join(MC2_nodes, by = c(\"source\" = \"id\")) %>%\n  rename(from = source) %>%\n  left_join(MC2_nodes, by = c(\"target\" = \"id\")) %>%\n  rename(to = target) %>%\n  filter(hscode==\"306170\" & Year==\"2028\")%>%\n  group_by(from, to) %>%\n    summarise(weight = n()) %>%\n  filter(from!=to) %>%\n  filter(weight > 20) %>%\n  ungroup()\nMC2_nodes_aggregated <- MC2_nodes %>%\n  filter(id %in% c(MC2_edges_aggregated$from, MC2_edges_aggregated$to))\n\n\n\nDT::datatable(MC2_nodes_aggregated, class= \"compact\")\n\n\n\n\n\n\n\nDT::datatable(MC2_edges_aggregated, class= \"compact\")\n\n\n\n\n\n\n\n\n2.4.2 Plot the graph\n\n\nShow the code\n#set.seed(123)\n#edges <- MC2_edges_aggregated #%>% sample_frac(0.005)\n#nodes <- MC2_nodes %>%\n  #filter(id %in% c(edges$from, edges$to))\nvisNetwork(MC2_nodes_aggregated,\n           MC2_edges_aggregated ) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\"))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualization-and-analysis",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualization-and-analysis",
    "title": "Take-home Exercise 2",
    "section": "3. Visualization and Analysis",
    "text": "3. Visualization and Analysis\n\nAn overview of the temporal change of sampled edges within 2028 to 2034\n\nHere we extract the edge data and group by year-month, and we can see the changes of the sum of weight and value_usd on a monthly changed base.Mostly the sum of weight is below 100k and value_usd below 0.6M.\n\n\nShow the code\nMC2_links$arrivalym <-  format(as.Date(MC2_links$arrivaldate),\"%Y-%m\")\nMC2_edges_5 <- MC2_links %>% group_by(source,target,arrivalym) %>% \n  summarise(count_hscode=n_distinct(hscode),\n            sum_omu=sum(valueofgoods_omu),\n            sum_teu=sum(volumeteu),\n            sum_weightkg=sum(weightkg),\n            sum_usd=sum(valueofgoodsusd),.groups = 'drop') %>%\n  \n  as.data.frame()\nMC2_edges_5 <- MC2_edges_5[complete.cases(MC2_edges_5[ , c('sum_weightkg', 'sum_usd')]), ]\nset.seed(123)\nMC2_nodes_5 <- MC2_nodes %>%\n  filter(id %in% MC2_edges_5$source) %>% sample_frac(0.005)\nMC2_edges_6 <- MC2_edges_5 %>%\n  filter(MC2_edges_5$source %in% MC2_nodes_5$id) %>%\n  filter(sum_weightkg< 2000000) %>%\n  filter(sum_usd< 1000000)\n\n  \nbp <- MC2_edges_6 %>%\n  plot_ly(x = ~sum_weightkg, \n          y = ~sum_usd, \n          size = ~sum_weightkg, \n          color = ~source,\n          sizes = c(2, 100),\n          frame = ~arrivalym, \n          text = ~target, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %>%\n  layout(showlegend = FALSE)\nbp\n\n\n\n\n\n\n\n\n\nComparison of distribution of mean avg_price records that have 1-7 years of history.\n\nHere we pay attention to a yearly change in below and we found that companies that has only 1 year of record has the highest avg_price. Considering if a company is legally fishing, mostly they will form a long relationship with collaborators and the price is reasonable. But for companies that are caught illegal fishing or shut down, the price are more likely to be higher than the market and their record histories are also short.\n\n\nShow the code\nMC2_edges <- MC2_links[complete.cases(MC2_links[ , c('hscode','valueofgoodsusd','volumeteu')]), ] %>% \n  filter(hscode == \"306170\") %>%\n  filter(weightkg != 0 ) %>% \n  mutate(price = valueofgoodsusd/weightkg)\n\nMC2_edges_1 <- MC2_edges %>% \n  group_by(source,target,Year) %>% \n  summarise(No_of_time=n(),\n            sum_omu=sum(valueofgoods_omu),\n            sum_teu=sum(volumeteu),\n            sum_weightkg=sum(weightkg),\n            sum_usd=sum(valueofgoodsusd),\n            mean_price=mean(price),.groups = 'drop') %>%\n  as.data.frame()\n\nMC2_edges_2 <- MC2_edges_1 %>% group_by(source,target) %>% \n  summarise(No_of_year=n_distinct(Year),\n            avg_not=mean(No_of_time),\n            avg_price=mean(mean_price),\n            avg_weightkg=mean(sum_weightkg),.groups = 'drop') %>%\n  #inner_join(edges, by = c(\"source\"=\"from\",\"target\"=\"to\")) %>%\n  as.data.frame()\n\n\nmy_sum <- MC2_edges_2 %>%\n  group_by(No_of_year) %>%\n  summarise(\n    n=n(),\n    mean=mean(avg_price),\n    sd=sd(avg_price)\n    ) %>%\n  mutate(se=sd/sqrt(n-1))\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=No_of_year, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=No_of_year, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean avg_price score by No_of_year\")\n\n\n\n\n\n\n\nStats table\n\n\nShow the code\nknitr::kable(my_sum, format = 'html')\n\n\n\n\n \n  \n    No_of_year \n    n \n    mean \n    sd \n    se \n  \n \n\n  \n    1 \n    4387 \n    10.93051 \n    4.400719 \n    0.0664491 \n  \n  \n    2 \n    1410 \n    10.70853 \n    2.746642 \n    0.0731723 \n  \n  \n    3 \n    713 \n    10.82606 \n    2.299127 \n    0.0861634 \n  \n  \n    4 \n    358 \n    10.66151 \n    1.886805 \n    0.0998603 \n  \n  \n    5 \n    250 \n    10.60475 \n    2.499360 \n    0.1583905 \n  \n  \n    6 \n    225 \n    10.58984 \n    1.293865 \n    0.0864500 \n  \n  \n    7 \n    207 \n    10.60920 \n    1.082008 \n    0.0753871 \n  \n\n\n\n\n\n\n\n\nFurther analysis on short-history edges\n\nHere we filter with year=1 and plot out 10%\n\n\nShow the code\nMC2_edges_2_aggregated <- MC2_edges_2 %>%\n  left_join(MC2_nodes, by = c(\"source\" = \"id\")) %>%\n  rename(from = source) %>%\n  left_join(MC2_nodes, by = c(\"target\" = \"id\")) %>%\n  rename(to = target) %>%\n  filter(No_of_year==1)%>%\n  filter(from!=to) %>%\n  filter(avg_not>5)%>%\n  ungroup()\nMC2_nodes_2_aggregated <- MC2_nodes %>%\n  filter(id %in% c(MC2_edges_2_aggregated$from, MC2_edges_2_aggregated$to))\n\nvisNetwork(MC2_nodes_2_aggregated,\n           MC2_edges_2_aggregated ) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\"))\n\n\n\n\n\n\n\n\nthe top 10% vertices based on their eigenvector centrality score\n\n\nShow the code\ncgraph <- graph_from_data_frame (MC2_edges_2_aggregated,\n                           vertices = MC2_nodes_2_aggregated) %>%\n  as_tbl_graph()\nquantile_graph <- quantile(eigen_centrality(cgraph)$vector,\n         probs = seq(0, 1, 1/10)\n         )\nV(cgraph)$size = eigen_centrality(cgraph)$vector\n\ncgraph_aggregated <- delete_vertices(cgraph, V(cgraph)[size < quantile_graph[10]])\n\n\nset.seed (1234)\nlayout1 <- layout_with_fr(cgraph_aggregated)\n\nquantile_graph_aggregated <- quantile(V(cgraph_aggregated)$size, #identify top 10% of the new vertices\n         probs = seq(0, 1, 1/10)\n         )\n\n\nV(cgraph_aggregated)$color <- ifelse (V(cgraph_aggregated)$size > quantile_graph_aggregated[10], \"darkgoldenrod3\", \"azure3\") #color yellow if vertices is top 10%\nE(cgraph_aggregated)$color <- \"grey\"\nV(cgraph_aggregated)$size <- V(cgraph_aggregated)$size/0.065 \n#Increase the size of nodes based on their centrality score, only those with high score will be visible\n\nV(cgraph_aggregated)$label <- ifelse (V(cgraph_aggregated)$size*0.065 > quantile_graph_aggregated[10],V(cgraph_aggregated)$name,NA)\n#label the vertices if vertices belongs to the top 10%\n\nplot(cgraph_aggregated, edge.arrow.size=0.25,edge.arrow.mode = \"-\", vertex.label = V(cgraph_aggregated)$label, vertex.label.cex = 0.65, vertex.label.font = 1, main = \"Which IDs has the most shipments within 1 year?\" ) \n\n\n\n\n\n\n\nHere we have the attributes of the top 10% vertices based on the updated EigenVector Centrality.\n\n\nShow the code\nvertex_attr(cgraph_aggregated, index = V(cgraph_aggregated)$size*0.065 > quantile_graph_aggregated[10])\n\n\n$name\n[1] \"hǎi dǎn Corporation Wharf\" \"Mar del Este CJSC\"        \n[3] \"Caracola del Sol Services\" \"Coral del Sur Sagl Family\"\n\n$shpcountry\n[1] \"Marebak\"     \"Merigrad\"    \"Puerto Sol\"  \"Solovarossa\"\n\n$rcvcountry\n[1] \"Oceanus\" \"Oceanus\" \"Oceanus\" \"Oceanus\"\n\n$size\n[1]  7.920180 15.384615 12.956742  8.635514\n\n$color\n[1] \"darkgoldenrod3\" \"darkgoldenrod3\" \"darkgoldenrod3\" \"darkgoldenrod3\"\n\n$label\n[1] \"hǎi dǎn Corporation Wharf\" \"Mar del Este CJSC\"        \n[3] \"Caracola del Sol Services\" \"Coral del Sur Sagl Family\"\n\n\n\n\ndetect possible cluster based on the betweeness score of the vertices\n\n\nShow the code\nGNC <- cluster_edge_betweenness(cgraph_aggregated, weights = NULL)\nV(cgraph_aggregated)$color <-membership(GNC)              \ncgraph_aggregated$palette <- diverging_pal(length(GNC)) \nplot(cgraph_aggregated, edge.arrow.size=0.25,edge.arrow.mode = \"-\", vertex.label = NA, main = \"How many Social Groups within the category?\")\n\n\n\n\n\n\n\nHere according to the results above, we can make an hypothesis that since FishEye knows from past experience that companies caught fishing illegally will shut down but will then often start up again under a different name, and we find out that companies that have only 1 year of transactions history ofter have the highest avg_price, it is very likely that among these, the trade records may have problems. And we focus on top 10% as it is weird that these companies disappeared in the second year if they fish legally since the trade record is more frequent. These companies and records should be well-studied to further find out more IUU fishing behaviors."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "title": "Hands-on Exercise 7",
    "section": "",
    "text": "pacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)\n\n\n\n\n\nattacks <- read_csv(\"data/eventlog.csv\")\n\nRows: 199999 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): source_country, tz\ndttm (1): timestamp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\n\n\n\n\n\n\nmake_hr_wkday <- function(ts, sc, tz) {\n  real_times <- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt <- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\n\n\n\nwkday_levels <- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks <- attacks %>%\n  group_by(tz) %>%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %>% \n  ungroup() %>% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\n\n\ngrouped <- attacks %>% \n  count(wkday, hour) %>% \n  ungroup() %>%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\nattacks_by_country <- count(\n  attacks, source_country) %>%\n  mutate(percent = percent(n/sum(n))) %>%\n  arrange(desc(n))\n\n\n\n\n\ntop4 <- attacks_by_country$source_country[1:4]\ntop4_attacks <- attacks %>%\n  filter(source_country %in% top4) %>%\n  count(source_country, wkday, hour) %>%\n  ungroup() %>%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %>%\n  na.omit()\n\n\n\n\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\nair <- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n\n\nair$month <- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year <- year(ymd(air$`Month-Year`))\n\n\n\n\n\nVietnam <- air %>% \n  select(`Vietnam`, \n         month, \n         year) %>%\n  filter(year >= 2010)\n\n\n\n\n\nhline.data <- Vietnam %>% \n  group_by(month) %>%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\")\n\n\n\n\n\n\n\n\n\n\n\nrice <- read_csv(\"data/rice.csv\")\n\nRows: 550 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Country\ndbl (3): Year, Yield, Production\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\nrice %>% \n  mutate(Year = factor(Year)) %>%\n  filter(Year %in% c(1961, 1980)) %>%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\nConverting 'Year' to an ordered factor"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started-1",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started-1",
    "title": "Hands-on Exercise 7",
    "section": "Getting started",
    "text": "Getting started\n\npacman::p_load(ggHoriPlot, ggthemes, tidyverse)\n\n\nStep 1: Data Import\n\naverp <- read_csv(\"data/AVERP.csv\") %>%\n  mutate(`Date` = dmy(`Date`))\n\nRows: 7452 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Date, Consumer Items\ndbl (1): Values\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nStep 2: Plotting the horizon graph\n\naverp %>% \n  filter(Date >= \"2018-01-01\") %>%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')"
  }
]